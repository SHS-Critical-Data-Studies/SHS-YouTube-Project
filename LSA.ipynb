{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import scipy.sparse\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "en_stopwords.update([s.capitalize() for s in stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Comments preprocessing pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comment preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Complete preprocessing\n",
    "def preprocess(comment):\n",
    "    \"\"\"\n",
    "    Preprocess a comment :\n",
    "        - remove numbers\n",
    "        - lower first letter of each sentence\n",
    "        - remove stopwords and words of 1 letter\n",
    "        - lower the words entirely in capital\n",
    "        - lemmatize\n",
    "        - remove again stopwords and words of 1 letter\n",
    "    :param comment: string containing the comment\n",
    "    :return: list of the tokens\n",
    "    \"\"\"\n",
    "    # Remove numbers\n",
    "    comment = re.sub(r'\\d+', '', comment)\n",
    "\n",
    "    # Lower first letter of each sentence\n",
    "    lower_first_word = lambda tab: ' '.join(tab[0].lower() + tab[1:])\n",
    "    comment = ' '.join([lower_first_word(sentence.split(' ')) for sentence in comment.split('.')])\n",
    "\n",
    "    # Tokenize by word\n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "    words_tokens = tokenizer.tokenize(comment)\n",
    "\n",
    "    # Remove stopwords and words of length 1\n",
    "    remove_stopwords = lambda wts: [w for w in wts if (not w in en_stopwords) and len(w) > 1]\n",
    "    words_tokens = remove_stopwords(words_tokens)\n",
    "\n",
    "    # Lower capital words\n",
    "    for i in range(len(words_tokens)):\n",
    "        if words_tokens[i].isupper():\n",
    "            words_tokens[i] = words_tokens[i].lower()\n",
    "\n",
    "    # Lemmatization with WordNet\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words_tokens = [lemmatizer.lemmatize(wt) for wt in words_tokens]\n",
    "\n",
    "    # Remove stopwords and words of length 1\n",
    "    words_tokens = remove_stopwords(words_tokens)\n",
    "\n",
    "    return words_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Preprocessor and tokenizer to be used directly in TfidfVectorizer\n",
    "def preprocessor(comment):\n",
    "    \"\"\"\n",
    "    Preprocess a comment:\n",
    "        - remove numbers\n",
    "        - lower first letter of each sentence\n",
    "    :param comment: string containing the comment\n",
    "    :return: string containing the preprocessed comment\n",
    "    \"\"\"\n",
    "    # Remove numbers\n",
    "    comment = re.sub(r'\\d+', '', comment)\n",
    "\n",
    "    # Lower first letter of each sentence\n",
    "    lower_first_word = lambda tab: ' '.join([tab[0].lower()] + tab[1:])\n",
    "    comment = ' '.join([lower_first_word(sentence.split(' ')) for sentence in comment.split('.')])\n",
    "\n",
    "    return comment\n",
    "\n",
    "def tokenizer(comment):\n",
    "    \"\"\"\n",
    "    Tokenize and process the tokens of a comment :\n",
    "        - tokenize the comment by word\n",
    "        - remove stopwords and words of 1 letter\n",
    "        - lower the words entirely in capital\n",
    "        - lemmatize\n",
    "        - remove stopwords and words of 1 letter\n",
    "    :param comment: string containing the comment\n",
    "    :return: list of tokens for this comment\n",
    "    \"\"\"\n",
    "    # Tokenize by mot\n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "    words_tokens = tokenizer.tokenize(comment)\n",
    "\n",
    "    # Remove stopwords and words of lenght 1\n",
    "    remove_stopwords = lambda wts: [w for w in wts if (not w in en_stopwords) and len(w) > 1]\n",
    "    words_tokens = remove_stopwords(words_tokens)\n",
    "\n",
    "    # Lower capital words\n",
    "    for i in range(len(words_tokens)):\n",
    "        if words_tokens[i].isupper():\n",
    "            words_tokens[i] = words_tokens[i].lower()\n",
    "\n",
    "    # Lemmatization with WordNet\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words_tokens = [lemmatizer.lemmatize(wt) for wt in words_tokens]\n",
    "\n",
    "    # Remove stopwords and words of lenght 1\n",
    "    words_tokens = remove_stopwords(words_tokens)\n",
    "\n",
    "    return words_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TF-IDF Matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_40331/2671406338.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m# Transform the comments corpus\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;31m# into a sparse TF-IDF matrix\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0mtfidf_matrix\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtfidf_vectorizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit_transform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;31m# Save the matrix\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/shs/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001B[0m in \u001B[0;36mfit_transform\u001B[0;34m(self, raw_documents, y)\u001B[0m\n\u001B[1;32m   2075\u001B[0m         \"\"\"\n\u001B[1;32m   2076\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_check_params\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2077\u001B[0;31m         \u001B[0mX\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit_transform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mraw_documents\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2078\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_tfidf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2079\u001B[0m         \u001B[0;31m# X is already a transformed view of raw_documents so\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/shs/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001B[0m in \u001B[0;36mfit_transform\u001B[0;34m(self, raw_documents, y)\u001B[0m\n\u001B[1;32m   1328\u001B[0m                     \u001B[0;32mbreak\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1329\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1330\u001B[0;31m         \u001B[0mvocabulary\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mX\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_count_vocab\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mraw_documents\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfixed_vocabulary_\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1331\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1332\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbinary\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/shs/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001B[0m in \u001B[0;36m_count_vocab\u001B[0;34m(self, raw_documents, fixed_vocab)\u001B[0m\n\u001B[1;32m   1218\u001B[0m             \u001B[0mvocabulary\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvocabulary\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1219\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mvocabulary\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1220\u001B[0;31m                 raise ValueError(\n\u001B[0m\u001B[1;32m   1221\u001B[0m                     \u001B[0;34m\"empty vocabulary; perhaps the documents only contain stop words\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1222\u001B[0m                 )\n",
      "\u001B[0;31mValueError\u001B[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "# Vectorizer for TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(preprocessor=preprocessor, tokenizer=tokenizer)\n",
    "\n",
    "# Transform the comments corpus\n",
    "# into a sparse TF-IDF matrix\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "# Save the matrix\n",
    "scipy.sparse.save_npz('data/tfidf_comments.npz', tfidf_matrix)\n",
    "\n",
    "# Save the vocabulary : dict(term: feature index)\n",
    "with open('data/vocabulary.pickle', 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer.vocabulary_, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Latent Semantic Analyis : SVD of the TF-IDF matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def project_SVD(X, dim=2, seed=0):\n",
    "    \"\"\"\n",
    "    Compute the truncated SVD of the matrix X, keeping\n",
    "    an approximation of rank dim (i.e. dim features\n",
    "    in the embedding)\n",
    "    :param X: TF-IDF sparse matrix\n",
    "    :param dim: rank of the truncated SVD\n",
    "    :param seed: seed for the random SVD\n",
    "    :return: Documents embedding, tokens embedding\n",
    "    \"\"\"\n",
    "    # Compute the truncated SVD\n",
    "    U, sigmas, Vt = randomized_svd(X, n_components=dim, random_state=seed)\n",
    "\n",
    "    # Tokens embeddings\n",
    "    X_emb = U @ np.diag(sigmas)\n",
    "\n",
    "    return X_emb, Vt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def print_features_description(Vt, index_map, top_num=5):\n",
    "    \"\"\"\n",
    "    Print the most important tokens for each\n",
    "    features of the tokens embeddings.\n",
    "    :param Vt: tokens embedding\n",
    "    :param index_map: dict(feature index: terme)\n",
    "    :param top_num: number of tokens to print\n",
    "    :return: -\n",
    "    \"\"\"\n",
    "    for i in range(Vt.shape[0]):\n",
    "        sord_idx = np.argsort(Vt[i])\n",
    "        top_min_idx = sord_idx[:top_num]\n",
    "        top_max_idx = sord_idx[::-1][:top_num]\n",
    "        print(f'\\nThe top {top_num} max values for feature {i} are:')\n",
    "        for index in top_max_idx:\n",
    "            print(f'{index_map[index]:<30} {Vt[i,index]:.4f}')\n",
    "\n",
    "        print(f'\\nThe top {top_num} min values for feature {i} are:')\n",
    "        for index in top_min_idx:\n",
    "            print(f'{index_map[index]:<30} {Vt[i,index]:.4f}')\n",
    "\n",
    "        print(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "X_emb, Vt = project_SVD(tfidf_matrix, dim=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "index_map = {v: k for k, v in tfidf_vectorizer.vocabulary_.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The top 5 max values for feature 0 are:\n",
      "Nantucket                      0.1891\n",
      "sea                            0.1854\n",
      "go                             0.1742\n",
      "passenger                      0.1570\n",
      "though                         0.1550\n",
      "\n",
      "The top 5 min values for feature 0 are:\n",
      "zephyr                         0.0270\n",
      "oriental                       0.0270\n",
      "outside                        0.0270\n",
      "difference                     0.0270\n",
      "palsied                        0.0270\n",
      "\n",
      "\n",
      "\n",
      "The top 5 max values for feature 1 are:\n",
      "Nantucket                      0.2433\n",
      "Euroclydon                     0.2155\n",
      "tempestuous                    0.1293\n",
      "thou                           0.1293\n",
      "window                         0.1293\n",
      "\n",
      "The top 5 min values for feature 1 are:\n",
      "sea                            -0.1535\n",
      "upon                           -0.1285\n",
      "passenger                      -0.1278\n",
      "image                          -0.0979\n",
      "broiled                        -0.0971\n",
      "\n",
      "\n",
      "\n",
      "The top 5 max values for feature 2 are:\n",
      "Nantucket                      0.3570\n",
      "first                          0.1752\n",
      "New                            0.1428\n",
      "whale                          0.1428\n",
      "Bedford                        0.1428\n",
      "\n",
      "The top 5 min values for feature 2 are:\n",
      "Euroclydon                     -0.2457\n",
      "tempestuous                    -0.1474\n",
      "thou                           -0.1474\n",
      "window                         -0.1474\n",
      "frost                          -0.0983\n",
      "\n",
      "\n",
      "\n",
      "The top 5 max values for feature 3 are:\n",
      "broiled                        0.1805\n",
      "passenger                      0.1477\n",
      "glory                          0.1203\n",
      "care                           0.1203\n",
      "begin                          0.1203\n",
      "\n",
      "The top 5 min values for feature 3 are:\n",
      "image                          -0.1601\n",
      "upon                           -0.1208\n",
      "score                          -0.1067\n",
      "robust                         -0.1067\n",
      "mile                           -0.1067\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_features_description(Vt, index_map)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}