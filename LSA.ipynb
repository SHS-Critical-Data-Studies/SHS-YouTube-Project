{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import scipy.sparse\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "en_stopwords.update([s.capitalize() for s in stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pipeline de preprocessing des commentaires"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def load_walk(path, mode):\n",
    "    \"\"\"\n",
    "    Load the data of a walk\n",
    "    :param path: path of the file\n",
    "    :param mode: what data to load\n",
    "    :return: dataframe containing the data of the walk\n",
    "    \"\"\"\n",
    "    use_cols = None\n",
    "    if mode == 'comments':\n",
    "        use_cols = range(1, 6)\n",
    "    elif mode == 'infos':\n",
    "        use_cols = range(1, 10)\n",
    "    else:\n",
    "        print('Mode not supported')\n",
    "        return\n",
    "    return pd.read_csv(path, compression='bz2', usecols=use_cols)\n",
    "\n",
    "\n",
    "def load_from_folder(folder, processing, mode):\n",
    "    \"\"\"\n",
    "    Load the comments of all the walks\n",
    "    present in the given folder\n",
    "    :param folder: folder containing the walks\n",
    "    :param processing: function that process each file of\n",
    "    the folder\n",
    "    :param mode: what data to load\n",
    "    :return: dataframe containing all the comments\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    walks = 0\n",
    "\n",
    "    with os.scandir(folder) as it:\n",
    "        for entry in it:\n",
    "            if entry.name.endswith(f'{mode}.csv.bz2') and entry.is_file():\n",
    "                df = processing(os.path.join(folder, entry.name), walks)\n",
    "                dfs.append(df)\n",
    "                walks += 1\n",
    "\n",
    "    return pd.concat(dfs)\n",
    "\n",
    "\n",
    "def load_all_walks_comments(folder, keep_en):\n",
    "    \"\"\"\n",
    "    Load the comments of all the walks\n",
    "    present in the given folder\n",
    "    :param folder: folder containing the walks\n",
    "    :param keep_en: only keeps english comments\n",
    "    :return: dataframe containing all the comments\n",
    "    \"\"\"\n",
    "    def processing(path, walk):\n",
    "        df = load_walk(path, 'comments')\n",
    "        if keep_en:\n",
    "            df = df.loc[df['text'].apply(lambda x: nlp(str(x))._.language['language'] == 'en'), :]\n",
    "        df['walk'] = walk\n",
    "        return df\n",
    "\n",
    "    return load_from_folder(folder, processing, 'comments')\n",
    "\n",
    "def load_all_walks_tags(folder, keep_en):\n",
    "    \"\"\"\n",
    "    Load the tags of all the walks\n",
    "    present in the given folder\n",
    "    :param folder: folder containing the walks\n",
    "    :param keep_en: only keeps english tags\n",
    "    :return: dataframe containing all the tags\n",
    "    \"\"\"\n",
    "    def processing(path, walk):\n",
    "        df = load_walk(path, 'infos')\n",
    "        def process(tags):\n",
    "            res = []\n",
    "            for t in tags[1:-1].replace(\"'\", '').split(', '):\n",
    "                if keep_en:\n",
    "                    if nlp(str(t))._.language['language'] == 'en':\n",
    "                        res.append(t)\n",
    "                else:\n",
    "                    res.append(t)\n",
    "\n",
    "            return ' '.join(res) if len(res) > 0 else np.nan\n",
    "\n",
    "        df['keywords'] = df['keywords'].apply(process)\n",
    "        df['walk'] = walk\n",
    "        return df\n",
    "\n",
    "    return load_from_folder(folder, processing, 'infos')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing d'un commentaire"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def preprocess(comment):\n",
    "    \"\"\"\n",
    "    Preprocess un commentaire :\n",
    "        - supprime les nombres\n",
    "        - passe la première lettre de chaque phrase en minuscule\n",
    "        - supprime les stopwords et mots de 1 lettre\n",
    "        - passe en minuscules les mots tout en majuscules\n",
    "        - applique une lemmatization\n",
    "        - supprime à nouveau les stopwords et mots de 1 lettre\n",
    "    :param comment: string contenant le commentaire\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Supprime les nombres\n",
    "    comment = re.sub(r'\\d+', '', comment)\n",
    "\n",
    "    # Première lettre de chaque phrase en minuscule\n",
    "    lower_first_word = lambda tab: ' '.join(tab[0].lower() + tab[1:])\n",
    "    comment = ' '.join([lower_first_word(sentence.split(' ')) for sentence in comment.split('.')])\n",
    "\n",
    "    # Tokenize par mot\n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "    words_tokens = tokenizer.tokenize(comment)\n",
    "\n",
    "    # Supprime les stopwords et mots de 1 lettre\n",
    "    remove_stopwords = lambda wts: [w for w in wts if (not w in en_stopwords) and len(w) > 1]\n",
    "    words_tokens = remove_stopwords(words_tokens)\n",
    "\n",
    "    # Passe les mots en majuscules en minuscules\n",
    "    for i in range(len(words_tokens)):\n",
    "        if words_tokens[i].isupper():\n",
    "            words_tokens[i] = words_tokens[i].lower()\n",
    "\n",
    "    # Lemmatization avec WordNet\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words_tokens = [lemmatizer.lemmatize(wt) for wt in words_tokens]\n",
    "\n",
    "    # Supprime les stopwords et mots de 1 lettre\n",
    "    words_tokens = remove_stopwords(words_tokens)\n",
    "\n",
    "    return words_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def preprocessor(comment):\n",
    "    \"\"\"\n",
    "    Preprocess un commentaire:\n",
    "        - supprime les nombres\n",
    "        - passe la première lettre de chaque phrase en minuscule\n",
    "    :param comment: commentaire à traiter\n",
    "    :return: commentaire traité\n",
    "    \"\"\"\n",
    "    # Supprime les nombres\n",
    "    comment = re.sub(r'\\d+', '', comment)\n",
    "\n",
    "    # Première lettre de chaque phrase en minuscule\n",
    "    lower_first_word = lambda tab: ' '.join([tab[0].lower()] + tab[1:])\n",
    "    comment = ' '.join([lower_first_word(sentence.split(' ')) for sentence in comment.split('.')])\n",
    "\n",
    "    return comment\n",
    "\n",
    "def tokenizer(comment):\n",
    "    \"\"\"\n",
    "    Tokenize et process les tokens d'un commentaire\n",
    "    :param comment: commentaire à traiter\n",
    "    :return: list des tokens pour ce commentaire\n",
    "    \"\"\"\n",
    "    # Tokenize par mot\n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "    words_tokens = tokenizer.tokenize(comment)\n",
    "\n",
    "    # Supprime les stopwords et mots de 1 lettre\n",
    "    remove_stopwords = lambda wts: [w for w in wts if (not w in en_stopwords) and len(w) > 1]\n",
    "    words_tokens = remove_stopwords(words_tokens)\n",
    "\n",
    "    # Passe les mots en majuscules en minuscules\n",
    "    for i in range(len(words_tokens)):\n",
    "        if words_tokens[i].isupper():\n",
    "            words_tokens[i] = words_tokens[i].lower()\n",
    "\n",
    "    # Lemmatization avec WordNet\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words_tokens = [lemmatizer.lemmatize(wt) for wt in words_tokens]\n",
    "\n",
    "    # Supprime les stopwords et mots de 1 lettre\n",
    "    words_tokens = remove_stopwords(words_tokens)\n",
    "\n",
    "    return words_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Matrice TF-IDF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# Load data\n",
    "walks_folder = 'data/P3'\n",
    "walk_path = 'all_p3'\n",
    "#data = load_all_walks_comments(walks_folder, True)\n",
    "data = load_all_walks_tags(walks_folder, True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Drop the comments where nan occurs\n",
    "data = data.dropna(subset='keywords')\n",
    "comments = data['keywords'].tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Vectorizer pour TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(preprocessor=preprocessor, tokenizer=tokenizer)\n",
    "\n",
    "# Transforme le corpus de commentaires\n",
    "# en une matrice sparse\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "# Sauvegarde de la matrice\n",
    "scipy.sparse.save_npz('data/tfidf_comments.npz', tfidf_matrix)\n",
    "\n",
    "# Sauvegarde du vocabulaire : dict(term: feature index)\n",
    "with open('data/vocabulary.pickle', 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer.vocabulary_, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Latent Semantic Analyis : SVD de la matrice TF-IDF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def project_SVD(X, dim=2, seed=0):\n",
    "    \"\"\"\n",
    "    Calcule la SVD tronquée de la matrice TF-IDF en\n",
    "    gardant une approximation de rang 'dim' (i.e.\n",
    "    le nombre de features de l'embedding).\n",
    "    :param X: TF-IDF sparse matrice\n",
    "    :param dim: rang de la SVD tronquée\n",
    "    :param seed: seed for the random SVD\n",
    "    :return: Embedding des documents, embedding des tokens\n",
    "    \"\"\"\n",
    "    # Calcule la matrice tronquée\n",
    "    U, sigmas, Vt = randomized_svd(X, n_components=dim, random_state=seed)\n",
    "\n",
    "    # Embedding des tokens\n",
    "    X_emb = U @ np.diag(sigmas)\n",
    "\n",
    "    return X_emb, Vt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def print_features_description(Vt, index_map, top_num=5):\n",
    "    \"\"\"\n",
    "    Affiche les tokens les plus (et moins) importants\n",
    "    pour chaque features de l'embedding des tokens\n",
    "    :param Vt: embedding des tokens\n",
    "    :param index_map: dict(feature index: terme)\n",
    "    :param top_num: nombre de mots à afficher\n",
    "    :return: -\n",
    "    \"\"\"\n",
    "    for i in range(Vt.shape[0]):\n",
    "        sord_idx = np.argsort(Vt[i])\n",
    "        top_min_idx = sord_idx[:top_num]\n",
    "        top_max_idx = sord_idx[::-1][:top_num]\n",
    "        print(f'\\nThe top {top_num} max values for feature {i} are:')\n",
    "        for index in top_max_idx:\n",
    "            print(f'{index_map[index]:<30} {Vt[i,index]:.4f}')\n",
    "\n",
    "        print(f'\\nThe top {top_num} min values for feature {i} are:')\n",
    "        for index in top_min_idx:\n",
    "            print(f'{index_map[index]:<30} {Vt[i,index]:.4f}')\n",
    "\n",
    "        print(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "X_emb, Vt = project_SVD(tfidf_matrix, dim=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "index_map = {v: k for k, v in tfidf_vectorizer.vocabulary_.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The top 5 max values for feature 0 are:\n",
      "Nantucket                      0.1891\n",
      "sea                            0.1854\n",
      "go                             0.1742\n",
      "passenger                      0.1570\n",
      "though                         0.1550\n",
      "\n",
      "The top 5 min values for feature 0 are:\n",
      "zephyr                         0.0270\n",
      "oriental                       0.0270\n",
      "outside                        0.0270\n",
      "difference                     0.0270\n",
      "palsied                        0.0270\n",
      "\n",
      "\n",
      "\n",
      "The top 5 max values for feature 1 are:\n",
      "Nantucket                      0.2433\n",
      "Euroclydon                     0.2155\n",
      "tempestuous                    0.1293\n",
      "thou                           0.1293\n",
      "window                         0.1293\n",
      "\n",
      "The top 5 min values for feature 1 are:\n",
      "sea                            -0.1535\n",
      "upon                           -0.1285\n",
      "passenger                      -0.1278\n",
      "image                          -0.0979\n",
      "broiled                        -0.0971\n",
      "\n",
      "\n",
      "\n",
      "The top 5 max values for feature 2 are:\n",
      "Nantucket                      0.3570\n",
      "first                          0.1752\n",
      "New                            0.1428\n",
      "whale                          0.1428\n",
      "Bedford                        0.1428\n",
      "\n",
      "The top 5 min values for feature 2 are:\n",
      "Euroclydon                     -0.2457\n",
      "tempestuous                    -0.1474\n",
      "thou                           -0.1474\n",
      "window                         -0.1474\n",
      "frost                          -0.0983\n",
      "\n",
      "\n",
      "\n",
      "The top 5 max values for feature 3 are:\n",
      "broiled                        0.1805\n",
      "passenger                      0.1477\n",
      "glory                          0.1203\n",
      "care                           0.1203\n",
      "begin                          0.1203\n",
      "\n",
      "The top 5 min values for feature 3 are:\n",
      "image                          -0.1601\n",
      "upon                           -0.1208\n",
      "score                          -0.1067\n",
      "robust                         -0.1067\n",
      "mile                           -0.1067\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_features_description(Vt, index_map)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}